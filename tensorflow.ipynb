{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmanueleCannizzaro/google_colaboratory/blob/master/tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4fSjP0BOXFx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fa14e989-bbac-4d51-f33b-6c706637b05c"
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import SGD,RMSprop #Stochastic gradient descent optimizer.5y \n",
        " \n",
        "batch_size = 128\n",
        "#10 numbers 0 to 9\n",
        "num_classes = 10\n",
        "#iterations for training with the training set.\n",
        "epochs = 30\n",
        " \n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        " \n",
        "#Convert the image pixils 28X28 to a single vector 784 so a training set\n",
        "#becomes a matrix. This is using numpy.reshape\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        " \n",
        "#Casting the number into float\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        " \n",
        "print('The first label from the traing set: ', y_train[0])\n",
        " \n",
        "#Compress the greyscale level from 0-225 to 0-1\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        " \n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')\n",
        " \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        " \n",
        "print('The first label in the training set is converted to ', y_train[0])\n",
        " \n",
        "#Create a model which contains mutliple layers\n",
        "model = Sequential()\n",
        " \n",
        "#Add a layer type Dense with 512 output units for the hidden layer\n",
        "#Because this is the input layer, we need to tell Keras what\n",
        "#the input data looks like in dimension\n",
        "#in this case, it is just a single dimension array with 784 units mapped to all\n",
        "#pixils in a 28X28 grey scale\n",
        "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
        " \n",
        "#According to the doc, dropout is used for preventing overfitting so it is\n",
        "#a regularisation process. It is easier in Keras than in Matlab\n",
        "model.add(Dropout(0.2))\n",
        " \n",
        "#Sigmoid function is used here, but it is said to use Relu function to have a\n",
        "#better performance. Sigmoid is a bit classic and old school feels like.\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        " \n",
        "model.summary()\n",
        "# Setting up the model for traing by defining the cost function which for is the loss param\n",
        "# optimiser which is how we use to find the minmal of the cost function\n",
        "# I use Stochastic gradient descent here as that is what I learnt from the course but there are more advanced optimisers\n",
        "# in Keras like RMSprop\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])#It looks like accuracy is the one we normally use\n",
        " \n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))\n",
        "#Like what I learnt from the course, we use training set and test set for training and\n",
        "#evaluating the performance \n",
        " \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 03:16:06.536836 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0808 03:16:06.567167 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0808 03:16:06.574739 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0808 03:16:06.591209 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0808 03:16:06.599185 140145737643904 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0808 03:16:06.665752 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0808 03:16:06.677277 140145737643904 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The first label from the traing set:  5\n",
            "60000 train samples\n",
            "10000 test samples\n",
            "The first label in the training set is converted to  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               401920    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 669,706\n",
            "Trainable params: 669,706\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0808 03:16:06.792224 140145737643904 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 6s 94us/step - loss: 0.2483 - acc: 0.9228 - val_loss: 0.1139 - val_acc: 0.9651\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1029 - acc: 0.9691 - val_loss: 0.0852 - val_acc: 0.9745\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0747 - acc: 0.9779 - val_loss: 0.1157 - val_acc: 0.9670\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0588 - acc: 0.9818 - val_loss: 0.0846 - val_acc: 0.9756\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0494 - acc: 0.9848 - val_loss: 0.0861 - val_acc: 0.9779\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0446 - acc: 0.9871 - val_loss: 0.0863 - val_acc: 0.9791\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0399 - acc: 0.9878 - val_loss: 0.0757 - val_acc: 0.9825\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0347 - acc: 0.9899 - val_loss: 0.0753 - val_acc: 0.9825\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0318 - acc: 0.9905 - val_loss: 0.0881 - val_acc: 0.9817\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0282 - acc: 0.9919 - val_loss: 0.0847 - val_acc: 0.9830\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.0906 - val_acc: 0.9823\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0238 - acc: 0.9933 - val_loss: 0.1012 - val_acc: 0.9815\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0239 - acc: 0.9933 - val_loss: 0.0968 - val_acc: 0.9821\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0216 - acc: 0.9937 - val_loss: 0.0883 - val_acc: 0.9841\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0216 - acc: 0.9940 - val_loss: 0.0979 - val_acc: 0.9855\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0205 - acc: 0.9943 - val_loss: 0.1108 - val_acc: 0.9837\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0202 - acc: 0.9947 - val_loss: 0.1077 - val_acc: 0.9841\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0189 - acc: 0.9951 - val_loss: 0.1043 - val_acc: 0.9839\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0188 - acc: 0.9949 - val_loss: 0.1081 - val_acc: 0.9839\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0166 - acc: 0.9955 - val_loss: 0.1069 - val_acc: 0.9831\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0176 - acc: 0.9957 - val_loss: 0.0962 - val_acc: 0.9851\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0160 - acc: 0.9958 - val_loss: 0.1233 - val_acc: 0.9818\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0154 - acc: 0.9961 - val_loss: 0.1277 - val_acc: 0.9834\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0155 - acc: 0.9961 - val_loss: 0.1147 - val_acc: 0.9830\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.1124 - val_acc: 0.9842\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0152 - acc: 0.9964 - val_loss: 0.1345 - val_acc: 0.9824\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0154 - acc: 0.9964 - val_loss: 0.1249 - val_acc: 0.9833\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 2s 30us/step - loss: 0.0157 - acc: 0.9963 - val_loss: 0.1167 - val_acc: 0.9840\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0132 - acc: 0.9965 - val_loss: 0.1267 - val_acc: 0.9837\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 2s 29us/step - loss: 0.0137 - acc: 0.9967 - val_loss: 0.1223 - val_acc: 0.9844\n",
            "Test loss: 0.12234407697346544\n",
            "Test accuracy: 0.9844\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}